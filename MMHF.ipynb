{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:00:58.451548Z",
     "start_time": "2025-07-15T07:00:40.959641Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dongzj\\.conda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device         : cuda:1\n",
      "label_file     : adni_dataset/ADNI_902.csv\n",
      "mri_dir        : adni_dataset/MRI\n",
      "pet_dir        : adni_dataset/PET\n",
      "table_dir      : adni_dataset/ADNI_Tabel.csv\n",
      "tabular_emb    : models/tabular_emb.csv\n",
      "AAL_dir        : adni_dataset/AAL_space-MNI152NLin6_res-2x2x2/AAL.nii\n",
      "table_startcol : 4\n",
      "task           : SMCIPMCI\n",
      "augment        : False\n",
      "split_ratio_test: 0.2\n",
      "seed           : 42\n",
      "num_epochs     : 100\n",
      "batch_size     : 4\n",
      "lr             : 0.0001\n",
      "weight_decay   : 1e-05\n",
      "fp16           : True\n",
      "checkpoint_dir : checkpoints\n",
      "nb_class       : 2\n",
      "n_splits       : 5\n",
      "dropout_rate   : 0.5\n",
      "in_channels    : 2\n",
      "seg_task       : False\n",
      "img_dim        : 512\n",
      "tab_dim        : 192\n"
     ]
    }
   ],
   "source": [
    "import os, json, time, csv, numpy as np, pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datasets.ADNI import ADNI, ADNI_transform\n",
    "from monai.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from utils.metrics import calculate_metrics\n",
    "\n",
    "# -------------------- 配置 --------------------\n",
    "def load_cfg(path):\n",
    "    with open(path) as f: \n",
    "        return json.load(f)\n",
    "\n",
    "class Cfg:\n",
    "    def __init__(self, d):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        for k, v in d.items(): \n",
    "            setattr(self, k, v)\n",
    "# ----------------- 加载配置 -------------------\n",
    "config_path = \"config/config.json\"\n",
    "cfg = Cfg(load_cfg(config_path))\n",
    "for name, val in vars(cfg).items():\n",
    "    print(f\"{name:15s}: {val}\")\n",
    "writer = SummaryWriter(cfg.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0358a4edfbf4b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ADNI Dataset: SMCIPMCI] 样本分布：\n",
      "  SMCI (0): 321\n",
      "  PMCI (1): 158\n",
      "\n",
      "Fold 1: train 335, val 48, test 96\n",
      "Fold 2: train 335, val 48, test 96\n",
      "Fold 3: train 335, val 48, test 96\n",
      "Fold 4: train 335, val 48, test 96\n",
      "Fold 5: train 336, val 48, test 95\n",
      "Fold indices saved to checkpoints\\fold_indices.json\n"
     ]
    }
   ],
   "source": [
    "full_dataset = ADNI(cfg.label_file, cfg.mri_dir, cfg.pet_dir, cfg.task, cfg.augment)\n",
    "full_ds      = full_dataset.data_dict         # list[dict]\n",
    "labels       = [d[\"label\"] for d in full_ds]\n",
    "\n",
    "# -------------------- 划分 --------------------\n",
    "\n",
    "fold_indices = defaultdict(dict)\n",
    "outer_cv = StratifiedKFold(\n",
    "    n_splits=cfg.n_splits, shuffle=True, random_state=cfg.seed\n",
    ")\n",
    "\n",
    "for fold, (train_val_idx, test_idx) in enumerate(outer_cv.split(full_ds, labels), start=1):\n",
    "    # ——— 内层 90/10 再分验证集 ———\n",
    "    train_val_labels = [labels[i] for i in train_val_idx]\n",
    "    idxs_inner       = np.arange(len(train_val_idx))\n",
    "    train_idx_in, val_idx_in = train_test_split(\n",
    "        idxs_inner, test_size=0.125, stratify=train_val_labels, random_state=cfg.seed\n",
    "    )\n",
    "\n",
    "    # ——— 映射回 full_ds 的绝对索引 ———\n",
    "    train_idx = np.array(train_val_idx)[train_idx_in]\n",
    "    val_idx   = np.array(train_val_idx)[val_idx_in]\n",
    "\n",
    "    fold_indices[fold][\"train_idx\"] = train_idx.tolist()\n",
    "    fold_indices[fold][\"val_idx\"]   = val_idx.tolist()\n",
    "    fold_indices[fold][\"test_idx\"]  = test_idx.tolist()\n",
    "\n",
    "    print(f\"Fold {fold}: train {len(train_idx)}, val {len(val_idx)}, test {len(test_idx)}\")\n",
    "\n",
    "# -------------------- 保存 JSON --------------------\n",
    "\n",
    "os.makedirs(cfg.checkpoint_dir, exist_ok=True)\n",
    "json_path = os.path.join(cfg.checkpoint_dir, \"fold_indices.json\")\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump({str(k): v for k, v in fold_indices.items()}, f, indent=2)\n",
    "print(f\"Fold indices saved to {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843681b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 加载数据 ------------------\n",
    "import json, torch, os\n",
    "from torch.utils.data import DataLoader\n",
    "from monai.data import Dataset\n",
    "\n",
    "def load_fold_indices(json_path, fold):\n",
    "    with open(json_path) as f:\n",
    "        fold_dict = json.load(f)\n",
    "    return (fold_dict[str(fold)][\"train_idx\"],\n",
    "            fold_dict[str(fold)][\"val_idx\"],\n",
    "            fold_dict[str(fold)][\"test_idx\"])\n",
    "\n",
    "def get_dataloaders(cfg, fold, batch_size=4, num_workers=4):\n",
    "    # ---- ① 读完整列表 ----\n",
    "    full_ds = ADNI(cfg.label_file, cfg.mri_dir, cfg.pet_dir,\n",
    "                   cfg.task, augment=False).data_dict\n",
    "\n",
    "    # ---- ② 折分索引 ----\n",
    "    train_idx, val_idx, test_idx = load_fold_indices(\n",
    "        os.path.join(cfg.checkpoint_dir, \"fold_indices.json\"), fold)\n",
    "\n",
    "    # ---- ③ 指定 transform ----\n",
    "    tf_tr, tf_val = ADNI_transform(augment=False)   # 训增强 / 验无增强\n",
    "    tf_te = tf_val                                 # 测试同 val\n",
    "\n",
    "    # ---- ④ MONAI Dataset ----\n",
    "    ds_train = Dataset([full_ds[i] for i in train_idx], transform=tf_tr)\n",
    "    ds_val   = Dataset([full_ds[i] for i in val_idx],   transform=tf_val)\n",
    "    ds_test  = Dataset([full_ds[i] for i in test_idx],  transform=tf_te)\n",
    "\n",
    "    # ---- ⑤ DataLoader ----\n",
    "    loader_tr = DataLoader(ds_train, batch_size=batch_size,\n",
    "                           shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
    "    loader_val= DataLoader(ds_val,   batch_size=batch_size,\n",
    "                           shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    loader_te = DataLoader(ds_test,  batch_size=batch_size,\n",
    "                           shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    return loader_tr, loader_val, loader_te\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "801fbfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ADNI Dataset: SMCIPMCI] 样本分布：\n",
      "  SMCI (0): 321\n",
      "  PMCI (1): 158\n",
      "\n",
      "输入体积张量 : torch.Size([2, 1, 96, 112, 96])\n",
      "输出特征图 : torch.Size([2, 64, 96, 112, 96])\n",
      "展平向量 : torch.Size([2, 66060288])\n",
      "全局均值池化后 : torch.Size([2, 64])\n",
      "Subject IDs: ['013_S_4791', '016_S_4601']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models.unet3d import UNet3D_Feature           # 已改成输出 64-ch 特征图的版本\n",
    "\n",
    "# ---------- 0. 配置 ----------\n",
    "fold       = 1                 # 任选一折\n",
    "BATCH_SIZE = 2                 # 每批多少张影像\n",
    "device     = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ---------- 1. DataLoader ----------\n",
    "loader_tr, loader_val, loader_te = get_dataloaders(\n",
    "    cfg, fold, batch_size=BATCH_SIZE, num_workers=4)\n",
    "\n",
    "# 只需要一个 batch，直接取第一批即可\n",
    "batch = next(iter(loader_tr))                       # type: dict\n",
    "vol   = batch[\"MRI\"].to(device)                    # (B,1,96,112,96) 等\n",
    "print(\"输入体积张量 :\", vol.shape)\n",
    "\n",
    "# ---------- 2. 模型 ----------\n",
    "model = UNet3D_Feature(in_channels=1).to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    feat_map = model(vol)                          # (B,64,96,112,96)\n",
    "    feat_vec = feat_map.flatten(1)                 # (B, 64*96*112*96)\n",
    "\n",
    "print(\"输出特征图 :\", feat_map.shape)\n",
    "print(\"展平向量 :\",   feat_vec.shape)\n",
    "\n",
    "# 若后续想要全局 64 维向量，可用 GAP：\n",
    "gap_vec = feat_map.mean(dim=(2,3,4))               # (B,64)\n",
    "print(\"全局均值池化后 :\", gap_vec.shape)\n",
    "\n",
    "# 可选：查看该 batch 对应的受试者 ID\n",
    "print(\"Subject IDs:\", batch[\"Subject\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da896663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n",
      "\n",
      "[ADNI Dataset: SMCIPMCI] 样本分布：\n",
      "  SMCI (0): 321\n",
      "  PMCI (1): 158\n",
      "\n",
      "Fold1 Ep001 | TrL=0.6540 TrACC=0.654 TrSEN=0.009 TrSPE=0.973 TrAUC=0.498 || VaL=0.6370 VaACC=0.667 VaSEN=0.000 VaSPE=1.000 VaAUC=0.535 | time 173.1s\n",
      "✅ new best AUC=0.535\n",
      "Fold1 Ep002 | TrL=0.6372 TrACC=0.669 TrSEN=0.000 TrSPE=1.000 TrAUC=0.470 || VaL=0.6363 VaACC=0.667 VaSEN=0.000 VaSPE=1.000 VaAUC=0.537 | time 174.6s\n",
      "✅ new best AUC=0.537\n",
      "Fold1 Ep003 | TrL=0.6383 TrACC=0.669 TrSEN=0.000 TrSPE=1.000 TrAUC=0.462 || VaL=0.6373 VaACC=0.667 VaSEN=0.000 VaSPE=1.000 VaAUC=0.537 | time 180.9s\n",
      "Fold1 Ep004 | TrL=0.6378 TrACC=0.669 TrSEN=0.000 TrSPE=1.000 TrAUC=0.458 || VaL=0.6365 VaACC=0.667 VaSEN=0.000 VaSPE=1.000 VaAUC=0.539 | time 174.6s\n",
      "✅ new best AUC=0.539\n",
      "Fold1 Ep005 | TrL=0.6370 TrACC=0.669 TrSEN=0.000 TrSPE=1.000 TrAUC=0.474 || VaL=0.6371 VaACC=0.667 VaSEN=0.000 VaSPE=1.000 VaAUC=0.541 | time 174.3s\n",
      "✅ new best AUC=0.541\n",
      "Fold1 Ep006 | TrL=0.6381 TrACC=0.669 TrSEN=0.000 TrSPE=1.000 TrAUC=0.480 || VaL=0.6363 VaACC=0.667 VaSEN=0.000 VaSPE=1.000 VaAUC=0.541 | time 176.3s\n"
     ]
    }
   ],
   "source": [
    "import os, csv, time, torch, json, numpy as np\n",
    "from torch import nn\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "# ---------------- 0. 工具：指标计算 ----------------\n",
    "def calc_stats(y_true, y_pred, y_prob):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sen = tp / (tp + fn + 1e-6)                    # Recall / Sensitivity\n",
    "    spe = tn / (tn + fp + 1e-6)\n",
    "    auc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true))==2 else 0.5\n",
    "    return {'ACC':acc, 'SEN':sen, 'SPE':spe, 'AUC':auc}\n",
    "\n",
    "# ---------------- 1. 模型组件 ----------------\n",
    "from models.unet3d   import UNet3D_Feature\n",
    "from models.ROI_pol  import ROIPooling3D, ROIClassifier\n",
    "\n",
    "unet  = UNet3D_Feature(in_channels=1).to(cfg.device).eval()   # 冻结\n",
    "pool  = ROIPooling3D(cfg.AAL_dir).to(cfg.device).eval()\n",
    "\n",
    "# ---------------- 2. 五折交叉验证 ----------------\n",
    "os.makedirs(cfg.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "for fold in range(1, cfg.n_splits+1):\n",
    "    print(f\"\\n=== Fold {fold}/{cfg.n_splits} ===\")\n",
    "    loader_tr, loader_val, _ = get_dataloaders(cfg, fold, batch_size=4)\n",
    "\n",
    "    clf  = ROIClassifier().to(cfg.device)\n",
    "    opt  = torch.optim.AdamW(clf.parameters(), lr=1e-4)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "\n",
    "    csv_path = os.path.join(cfg.checkpoint_dir, f\"fold{fold}_metrics.csv\")\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow(\n",
    "            [\"epoch\",\"set\",\"Loss\",\"ACC\",\"SEN\",\"SPE\",\"AUC\"])\n",
    "\n",
    "    best_auc = -np.inf\n",
    "    for epoch in range(1, cfg.num_epochs+1):\n",
    "        t0 = time.time()\n",
    "        stats = {}\n",
    "        # ------- Train -------\n",
    "        clf.train()\n",
    "        loss_sum, yt, yp, ys = 0, [], [], []\n",
    "        for batch in loader_tr:\n",
    "            mri = batch[\"MRI\"].to(cfg.device)\n",
    "            y   = batch[\"label\"].to(cfg.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                feat = pool(unet(mri))           # (B,94,64)\n",
    "            logit = clf(feat)\n",
    "            loss  = crit(logit, y)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "            loss_sum += loss.item()*y.size(0)\n",
    "            # prob = torch.softmax(logit,1)[:,1].cpu().numpy()\n",
    "            prob = torch.softmax(logit, 1)[:, 1].detach().cpu().numpy()\n",
    "\n",
    "            pred = logit.argmax(1).cpu().numpy()\n",
    "            yt.extend(y.cpu().numpy()); yp.extend(pred); ys.extend(prob)\n",
    "\n",
    "        stats[\"train\"] = {\"Loss\":loss_sum/len(loader_tr.dataset),\n",
    "                          **calc_stats(yt, yp, ys)}\n",
    "\n",
    "        # ------- Val -------\n",
    "        clf.eval(); loss_sum, yt, yp, ys = 0, [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader_val:\n",
    "                mri = batch[\"MRI\"].to(cfg.device)\n",
    "                y   = batch[\"label\"].to(cfg.device)\n",
    "                feat= pool(unet(mri))\n",
    "                logit=clf(feat); loss=crit(logit,y)\n",
    "                loss_sum+=loss.item()*y.size(0)\n",
    "                prob = torch.softmax(logit, 1)[:, 1].detach().cpu().numpy()\n",
    "#                             ^^^^^^^  先分离计算图\n",
    "\n",
    "                pred=logit.argmax(1).cpu().numpy()\n",
    "                yt.extend(y.cpu().numpy()); yp.extend(pred); ys.extend(prob)\n",
    "        stats[\"val\"]={\"Loss\":loss_sum/len(loader_val.dataset),\n",
    "                      **calc_stats(yt,yp,ys)}\n",
    "\n",
    "        # ------- 打印 / 记录 -------\n",
    "        msg=(f\"Fold{fold} Ep{epoch:03d} | \"\n",
    "             f\"TrL={stats['train']['Loss']:.4f} \"\n",
    "             f\"TrACC={stats['train']['ACC']:.3f} \"\n",
    "             f\"TrSEN={stats['train']['SEN']:.3f} \"\n",
    "             f\"TrSPE={stats['train']['SPE']:.3f} \"\n",
    "             f\"TrAUC={stats['train']['AUC']:.3f} || \"\n",
    "             f\"VaL={stats['val']['Loss']:.4f} \"\n",
    "             f\"VaACC={stats['val']['ACC']:.3f} \"\n",
    "             f\"VaSEN={stats['val']['SEN']:.3f} \"\n",
    "             f\"VaSPE={stats['val']['SPE']:.3f} \"\n",
    "             f\"VaAUC={stats['val']['AUC']:.3f} | \"\n",
    "             f\"time {time.time()-t0:.1f}s\")\n",
    "        print(msg)\n",
    "\n",
    "        with open(csv_path,\"a\",newline=\"\") as f:\n",
    "            w=csv.writer(f)\n",
    "            for split in (\"train\",\"val\"):\n",
    "                w.writerow([epoch,split,\n",
    "                            f\"{stats[split]['Loss']:.4f}\",\n",
    "                            f\"{stats[split]['ACC']:.4f}\",\n",
    "                            f\"{stats[split]['SEN']:.4f}\",\n",
    "                            f\"{stats[split]['SPE']:.4f}\",\n",
    "                            f\"{stats[split]['AUC']:.4f}\"])\n",
    "\n",
    "        # ------- 保存最好 -------\n",
    "        if stats[\"val\"][\"AUC\"]>best_auc:\n",
    "            best_auc=stats[\"val\"][\"AUC\"]\n",
    "            torch.save(clf.state_dict(),\n",
    "                os.path.join(cfg.checkpoint_dir,f\"roi_clf_fold{fold}.pth\"))\n",
    "            print(f\"✅ new best AUC={best_auc:.3f}\")\n",
    "\n",
    "    print(f\"=== Fold {fold} done, best AUC={best_auc:.3f} ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a99f706b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n",
      "\n",
      "[ADNI Dataset: SMCIPMCI] 样本分布：\n",
      "  SMCI (0): 321\n",
      "  PMCI (1): 158\n",
      "\n",
      "Fold1 Ep001 | TrL=0.6713 TrAUC=0.527 || VaL=0.6456 VaAUC=0.516 time=324.1s\n",
      "✅ new best AUC = 0.516\n",
      "Fold1 Ep002 | TrL=0.6513 TrAUC=0.464 || VaL=0.6375 VaAUC=0.509 time=325.2s\n",
      "Fold1 Ep003 | TrL=0.6392 TrAUC=0.509 || VaL=0.6369 VaAUC=0.606 time=325.7s\n",
      "✅ new best AUC = 0.606\n",
      "Fold1 Ep004 | TrL=0.6498 TrAUC=0.451 || VaL=0.6366 VaAUC=0.514 time=325.9s\n",
      "Fold1 Ep005 | TrL=0.6424 TrAUC=0.484 || VaL=0.6365 VaAUC=0.516 time=326.0s\n",
      "Fold1 Ep006 | TrL=0.6328 TrAUC=0.540 || VaL=0.6365 VaAUC=0.609 time=325.9s\n",
      "✅ new best AUC = 0.609\n",
      "Fold1 Ep007 | TrL=0.6468 TrAUC=0.470 || VaL=0.6365 VaAUC=0.484 time=326.5s\n",
      "Fold1 Ep008 | TrL=0.6368 TrAUC=0.528 || VaL=0.6365 VaAUC=0.531 time=331.0s\n",
      "Fold1 Ep009 | TrL=0.6317 TrAUC=0.545 || VaL=0.6368 VaAUC=0.484 time=330.5s\n",
      "Fold1 Ep010 | TrL=0.6413 TrAUC=0.504 || VaL=0.6365 VaAUC=0.498 time=326.9s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m     f_mri \u001b[38;5;241m=\u001b[39m pool(unet_mri(mri))  \u001b[38;5;66;03m# [B,94,64]\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     f_pet \u001b[38;5;241m=\u001b[39m pool(unet_pet(pet))\n\u001b[1;32m---> 72\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[43mhg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_mri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_pet\u001b[49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# dict\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     fused \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([g[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal_mod1\u001b[39m\u001b[38;5;124m\"\u001b[39m], g[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal_mod2\u001b[39m\u001b[38;5;124m\"\u001b[39m]], \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B,128]\u001b[39;00m\n\u001b[0;32m     75\u001b[0m logit \u001b[38;5;241m=\u001b[39m clf(fused)\n",
      "File \u001b[1;32mc:\\Users\\dongzj\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dongzj\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dongzj\\Desktop\\Multimodal_AD\\models\\HGNN.py:173\u001b[0m, in \u001b[0;36mDualModalHyperGraphWithAttn.forward\u001b[1;34m(self, feat1, feat2)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, feat1, feat2):\n\u001b[1;32m--> 173\u001b[0m     o1, o2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat2\u001b[49m\u001b[43m)\u001b[49m     \u001b[38;5;66;03m# [B,N,H]\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     z1, z2, α12, α21 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(o1, o2)   \u001b[38;5;66;03m# [B,N,dk], [B,N]\u001b[39;00m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;66;03m# 全局聚合：∑ α_i · z_i\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dongzj\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dongzj\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dongzj\\Desktop\\Multimodal_AD\\models\\HGNN.py:113\u001b[0m, in \u001b[0;36mDualModalHyperGraph.forward\u001b[1;34m(self, feat_mod1, feat_mod2)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;03mfeat_mod1 / feat_mod2 : [B, N, C]\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    112\u001b[0m B, N, _ \u001b[38;5;241m=\u001b[39m feat_mod1\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 113\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeat_mod1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_mod2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# [2N,E]\u001b[39;00m\n\u001b[0;32m    115\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([feat_mod1, feat_mod2], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)               \u001b[38;5;66;03m# [B,2N,C]\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n",
      "File \u001b[1;32mc:\\Users\\dongzj\\.conda\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dongzj\\Desktop\\Multimodal_AD\\models\\HGNN.py:44\u001b[0m, in \u001b[0;36mHyperGraphIncidenceBuilder.__call__\u001b[1;34m(self, feats)\u001b[0m\n\u001b[0;32m     42\u001b[0m H_modal \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m midx, feat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(feats):\n\u001b[1;32m---> 44\u001b[0m     H_modal\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_modal_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmidx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# —— 模态间二元超边 —— #\u001b[39;00m\n\u001b[0;32m     47\u001b[0m N \u001b[38;5;241m=\u001b[39m feats[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dongzj\\.conda\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dongzj\\Desktop\\Multimodal_AD\\models\\HGNN.py:31\u001b[0m, in \u001b[0;36mHyperGraphIncidenceBuilder._build_modal_edges\u001b[1;34m(self, feat, k_set)\u001b[0m\n\u001b[0;32m     29\u001b[0m     H \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(N, N, device\u001b[38;5;241m=\u001b[39mfeat\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e_idx, nbrs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(knn):   \u001b[38;5;66;03m# N 条超边\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m         H[nbrs, e_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\n\u001b[0;32m     32\u001b[0m     H_list\u001b[38;5;241m.\u001b[39mappend(H)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(H_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dongzj\\.conda\\envs\\torch\\lib\\site-packages\\monai\\data\\meta_tensor.py:283\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 283\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "File \u001b[1;32mc:\\Users\\dongzj\\.conda\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py:1512\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m-> 1512\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[0;32m   1514\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------- 超图 ---------------------\n",
    "# main_train.py  --------------------------------------------------------------\n",
    "import os, csv, time, json, numpy as np\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "# ---------------- 1. 指标函数 ----------------\n",
    "def calc_stats(y_true, y_pred, y_prob):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sen = tp / (tp + fn + 1e-6)\n",
    "    spe = tn / (tn + fp + 1e-6)\n",
    "    auc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true))==2 else 0.5\n",
    "    return {'ACC':acc, 'SEN':sen, 'SPE':spe, 'AUC':auc}\n",
    "\n",
    "# ---------------- 2. 导入模型组件 ----------------\n",
    "from models.unet3d        import UNet3D_Feature\n",
    "from models.ROI_pol       import ROIPooling3D\n",
    "from models.HGNN    import DualModalHyperGraphWithAttn\n",
    "from models.fc_classifier import FCClassifier\n",
    "\n",
    "device = cfg.device\n",
    "# a) 影像编码器\n",
    "unet_mri = UNet3D_Feature(in_channels=1).to(device).eval()\n",
    "unet_pet = UNet3D_Feature(in_channels=1).to(device).eval()  # 可共享权重\n",
    "\n",
    "# b) ROI Pool\n",
    "pool = ROIPooling3D(cfg.AAL_dir).to(device).eval()\n",
    "\n",
    "# c) 超图融合模块\n",
    "hg = DualModalHyperGraphWithAttn(in_dim=64, hidden_dim=128,\n",
    "                                 num_layers=2, dk=64).to(device).eval()\n",
    "\n",
    "# d) 分类头（仅该模块参与训练）\n",
    "clf  = FCClassifier(in_dim=128, num_classes=2,\n",
    "                    hidden_dims=[256,128,64], p_drop=0.3).to(device)\n",
    "\n",
    "opt  = torch.optim.AdamW(clf.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "# ---------------- 3. 数据加载器 ----------------\n",
    "\n",
    "# 创建保存目录\n",
    "os.makedirs(cfg.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# ---------------- 4. 五折交叉验证 ----------------\n",
    "for fold in range(1, cfg.n_splits+1):\n",
    "    print(f\"\\n=== Fold {fold}/{cfg.n_splits} ===\")\n",
    "    loader_tr, loader_val, _ = get_dataloaders(cfg, fold, batch_size=cfg.batch_size)\n",
    "\n",
    "    best_auc = -np.inf\n",
    "    csv_path = os.path.join(cfg.checkpoint_dir, f\"fold{fold}_metrics.csv\")\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow(\n",
    "            [\"epoch\",\"set\",\"Loss\",\"ACC\",\"SEN\",\"SPE\",\"AUC\"])\n",
    "\n",
    "    for epoch in range(1, cfg.num_epochs+1):\n",
    "        t0 = time.time()\n",
    "        stats = {}\n",
    "\n",
    "        # ======== Train ========\n",
    "        clf.train()\n",
    "        loss_sum, yt, yp, ys = 0, [], [], []\n",
    "        for batch in loader_tr:\n",
    "            mri = batch[\"MRI\"].to(device)    # [B,1,D,H,W]\n",
    "            pet = batch[\"PET\"].to(device)\n",
    "            y   = batch[\"label\"].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                f_mri = pool(unet_mri(mri))  # [B,94,64]\n",
    "                f_pet = pool(unet_pet(pet))\n",
    "                g = hg(f_mri, f_pet)         # dict\n",
    "                fused = torch.cat([g[\"global_mod1\"], g[\"global_mod2\"]], 1)  # [B,128]\n",
    "\n",
    "            logit = clf(fused)\n",
    "            loss  = crit(logit, y)\n",
    "\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "            loss_sum += loss.item()*y.size(0)\n",
    "            prob = torch.softmax(logit,1)[:,1].detach().cpu().numpy()\n",
    "            pred = logit.argmax(1).cpu().numpy()\n",
    "            yt.extend(y.cpu().numpy()); yp.extend(pred); ys.extend(prob)\n",
    "\n",
    "        stats[\"train\"] = {\"Loss\":loss_sum/len(loader_tr.dataset),\n",
    "                          **calc_stats(yt,yp,ys)}\n",
    "\n",
    "        # ======== Valid ========\n",
    "        clf.eval(); loss_sum, yt, yp, ys = 0, [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader_val:\n",
    "                mri = batch[\"MRI\"].to(device)\n",
    "                pet = batch[\"PET\"].to(device)\n",
    "                y   = batch[\"label\"].to(device)\n",
    "\n",
    "                f_mri = pool(unet_mri(mri))\n",
    "                f_pet = pool(unet_pet(pet))\n",
    "                g = hg(f_mri, f_pet)\n",
    "                fused = torch.cat([g[\"global_mod1\"], g[\"global_mod2\"]], 1)\n",
    "\n",
    "                logit = clf(fused); loss = crit(logit, y)\n",
    "                loss_sum += loss.item()*y.size(0)\n",
    "                prob = torch.softmax(logit,1)[:,1].cpu().numpy()\n",
    "                pred = logit.argmax(1).cpu().numpy()\n",
    "                yt.extend(y.cpu().numpy()); yp.extend(pred); ys.extend(prob)\n",
    "\n",
    "        stats[\"val\"] = {\"Loss\":loss_sum/len(loader_val.dataset),\n",
    "                        **calc_stats(yt,yp,ys)}\n",
    "\n",
    "        # ======== 日志输出 ========\n",
    "        msg = (f\"Fold{fold} Ep{epoch:03d} | \"\n",
    "               f\"TrL={stats['train']['Loss']:.4f} \"\n",
    "               f\"TrAUC={stats['train']['AUC']:.3f} || \"\n",
    "               f\"VaL={stats['val']['Loss']:.4f} \"\n",
    "               f\"VaAUC={stats['val']['AUC']:.3f} \"\n",
    "               f\"time={time.time()-t0:.1f}s\")\n",
    "        print(msg)\n",
    "\n",
    "        with open(csv_path, \"a\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            for split in (\"train\",\"val\"):\n",
    "                w.writerow([epoch, split,\n",
    "                            f\"{stats[split]['Loss']:.4f}\",\n",
    "                            f\"{stats[split]['ACC']:.4f}\",\n",
    "                            f\"{stats[split]['SEN']:.4f}\",\n",
    "                            f\"{stats[split]['SPE']:.4f}\",\n",
    "                            f\"{stats[split]['AUC']:.4f}\"])\n",
    "\n",
    "        # ======== 保存最优 ========\n",
    "        if stats[\"val\"][\"AUC\"] > best_auc:\n",
    "            best_auc = stats[\"val\"][\"AUC\"]\n",
    "            torch.save(clf.state_dict(),\n",
    "                       os.path.join(cfg.checkpoint_dir,\n",
    "                                    f\"best_fold{fold}.pth\"))\n",
    "            print(f\"✅ new best AUC = {best_auc:.3f}\")\n",
    "\n",
    "    print(f\"=== Fold {fold} finished, best AUC = {best_auc:.3f} ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1763b4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 saved:\n",
      "  train → checkpoints\\fold1\\train.csv\n",
      "  val   → checkpoints\\fold1\\val.csv\n",
      "  test  → checkpoints\\fold1\\test.csv\n",
      "Fold 2 saved:\n",
      "  train → checkpoints\\fold2\\train.csv\n",
      "  val   → checkpoints\\fold2\\val.csv\n",
      "  test  → checkpoints\\fold2\\test.csv\n",
      "Fold 3 saved:\n",
      "  train → checkpoints\\fold3\\train.csv\n",
      "  val   → checkpoints\\fold3\\val.csv\n",
      "  test  → checkpoints\\fold3\\test.csv\n",
      "Fold 4 saved:\n",
      "  train → checkpoints\\fold4\\train.csv\n",
      "  val   → checkpoints\\fold4\\val.csv\n",
      "  test  → checkpoints\\fold4\\test.csv\n",
      "Fold 5 saved:\n",
      "  train → checkpoints\\fold5\\train.csv\n",
      "  val   → checkpoints\\fold5\\val.csv\n",
      "  test  → checkpoints\\fold5\\test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 假设 full_ds = ADNI(...).data_dict，cfg 已定义\n",
    "json_path = os.path.join(cfg.checkpoint_dir, \"fold_indices.json\")\n",
    "csv_path  = os.path.join(cfg.table_dir)  # 请替换为你的实际文件名\n",
    "\n",
    "# 读取完整表格\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 读取 fold 索引\n",
    "with open(json_path, 'r') as f:\n",
    "    fold_indices = json.load(f)\n",
    "\n",
    "# 提取 full_ds 中的 Subject_ID 列表\n",
    "all_subjects = [entry['Subject'] for entry in full_ds]\n",
    "\n",
    "for fold_str, idxs in fold_indices.items():\n",
    "    fold = int(fold_str)\n",
    "    train_idx = idxs['train_idx']\n",
    "    val_idx   = idxs['val_idx']\n",
    "    test_idx  = idxs['test_idx']\n",
    "\n",
    "    # 根据索引得到本折的 Subject_ID 列表\n",
    "    train_subs = [ all_subjects[i] for i in train_idx ]\n",
    "    val_subs   = [ all_subjects[i] for i in val_idx   ]\n",
    "    test_subs  = [ all_subjects[i] for i in test_idx  ]\n",
    "\n",
    "    # 在原始 df 中筛出对应行\n",
    "    df_train = df[df['Subject_ID'].isin(train_subs)].reset_index(drop=True)\n",
    "    df_val   = df[df['Subject_ID'].isin(val_subs)]  .reset_index(drop=True)\n",
    "    df_test  = df[df['Subject_ID'].isin(test_subs)] .reset_index(drop=True)\n",
    "\n",
    "    # 重排列顺序：第一列 Subject_ID，第二列 Group，后面是所有其他列\n",
    "    def reorder(df_split):\n",
    "        cols = df_split.columns.tolist()\n",
    "        cols.remove('Subject_ID')\n",
    "        if 'Group' not in cols:\n",
    "            raise KeyError(\"'Group' 列未找到，请确认表格中有此列\")\n",
    "        cols.remove('Group')\n",
    "        return df_split[['Subject_ID', 'Group'] + cols]\n",
    "\n",
    "    df_train = reorder(df_train)\n",
    "    df_val   = reorder(df_val)\n",
    "    df_test  = reorder(df_test)\n",
    "\n",
    "    # 保存到各自目录\n",
    "    out_dir = os.path.join(cfg.checkpoint_dir, f\"fold{fold}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    df_train.to_csv(os.path.join(out_dir, \"train.csv\"), index=False)\n",
    "    df_val  .to_csv(os.path.join(out_dir, \"val.csv\"),   index=False)\n",
    "    df_test .to_csv(os.path.join(out_dir, \"test.csv\"),  index=False)\n",
    "\n",
    "    print(f\"Fold {fold} saved:\")\n",
    "    print(f\"  train → {os.path.join(out_dir, 'train.csv')}\")\n",
    "    print(f\"  val   → {os.path.join(out_dir, 'val.csv')}\")\n",
    "    print(f\"  test  → {os.path.join(out_dir, 'test.csv')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64127f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Using device: cuda:0\n",
      "✓ Saved train_emb.csv ((335, 194))\n",
      "✓ Saved val_emb.csv ((48, 194))\n",
      "✓ Saved test_emb.csv ((96, 194))\n",
      "✓ Saved train_emb.csv ((335, 194))\n",
      "✓ Saved val_emb.csv ((48, 194))\n",
      "✓ Saved test_emb.csv ((96, 194))\n",
      "✓ Saved train_emb.csv ((335, 194))\n",
      "✓ Saved val_emb.csv ((48, 194))\n",
      "✓ Saved test_emb.csv ((96, 194))\n",
      "✓ Saved train_emb.csv ((335, 194))\n",
      "✓ Saved val_emb.csv ((48, 194))\n",
      "✓ Saved test_emb.csv ((96, 194))\n",
      "✓ Saved train_emb.csv ((336, 194))\n",
      "✓ Saved val_emb.csv ((48, 194))\n",
      "✓ Saved test_emb.csv ((95, 194))\n"
     ]
    }
   ],
   "source": [
    "from models.tabular_encoder import tabular_encoder_fold\n",
    "\n",
    "for fold in range(1, cfg.n_splits+1):\n",
    "    fold_dir = os.path.join(cfg.checkpoint_dir, f\"fold{fold}\")\n",
    "    if cfg.task == \"ADCN\":\n",
    "        classes = [\"CN\", \"AD\"]\n",
    "    elif cfg.task == \"SMCIPMCI\":\n",
    "        classes = [\"SMCI\", \"PMCI\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task: {cfg.task}\")\n",
    "    tabular_encoder_fold(\n",
    "        fold_dir    = fold_dir,\n",
    "        label_col   = \"Group\",\n",
    "        classes     = classes,\n",
    "        start_col   = 3,\n",
    "        device      = cfg.device,\n",
    "        n_fold      = 5,\n",
    "        dropna      = False\n",
    "    )\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87124989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 1. 建立 Subject ➜ 影像样本字典，便于快速对齐\n",
    "# -------------------------------------------------\n",
    "subject_map = {d[\"Subject\"]: d for d in full_ds}\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. 为五折构造 train/val/test DataLoader\n",
    "# -------------------------------------------------\n",
    "import pandas as pd\n",
    "from monai.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_emb_csv(path):\n",
    "    \"\"\"CSV -> {sid: (label:int, emb:numpy[192])}\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    sid   = df.iloc[:, 0].astype(str).tolist()\n",
    "    label = df.iloc[:, 1].astype(int).tolist()\n",
    "    emb   = df.iloc[:, 2:].astype(\"float32\").values\n",
    "    return {s: (l, e) for s, l, e in zip(sid, label, emb)}\n",
    "\n",
    "tr_tf, vl_tf = ADNI_transform(augment=cfg.augment)\n",
    "te_tf        = vl_tf\n",
    "\n",
    "fold_loaders = []\n",
    "\n",
    "for fold in range(1, cfg.n_splits + 1):\n",
    "    fold_dir  = os.path.join(cfg.checkpoint_dir, f\"fold{fold}\")\n",
    "    paths     = {sp: os.path.join(fold_dir, f\"{sp}_emb.csv\")\n",
    "                 for sp in [\"train\", \"val\", \"test\"]}\n",
    "\n",
    "    # 解析 CSV\n",
    "    emb_maps  = {sp: load_emb_csv(p) for sp, p in paths.items()}\n",
    "\n",
    "    split_ds  = {}\n",
    "    for sp, emb_map in emb_maps.items():\n",
    "        samples = []\n",
    "        for sid, (lbl, emb) in emb_map.items():\n",
    "            if sid not in subject_map:\n",
    "                raise KeyError(f\"{sid} not found in ADNI dataset\")\n",
    "            s = subject_map[sid].copy()     # MRI / PET / label / Subject\n",
    "            s[\"label\"] = lbl                # 以 CSV 为准\n",
    "            s[\"table\"] = emb                # 192‑d numpy\n",
    "            samples.append(s)\n",
    "        split_ds[sp] = samples\n",
    "\n",
    "    # DataLoader\n",
    "    dl_kw = dict(batch_size=cfg.batch_size, pin_memory=True)\n",
    "    fold_loaders.append({\n",
    "        \"fold\": fold,\n",
    "        \"train_loader\": DataLoader(Dataset(split_ds[\"train\"], tr_tf),\n",
    "                                   shuffle=True,  num_workers=4, **dl_kw),\n",
    "        \"val_loader\":   DataLoader(Dataset(split_ds[\"val\"],   vl_tf),\n",
    "                                   shuffle=False, num_workers=2, **dl_kw),\n",
    "        \"test_loader\":  DataLoader(Dataset(split_ds[\"test\"],  te_tf),\n",
    "                                   shuffle=False, num_workers=2, **dl_kw),\n",
    "    })\n",
    "\n",
    "    print(f\"Fold {fold}  ➜  train {len(split_ds['train'])} | \"\n",
    "          f\"val {len(split_ds['val'])} | test {len(split_ds['test'])}\")\n",
    "\n",
    "# 现在 fold_loaders 就和之前影像-only 版本一模一样可直接用于训练：\n",
    "# for fold_dict in fold_loaders:\n",
    "#     tr_loader = fold_dict[\"train_loader\"]\n",
    "#     ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946f892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- 创建模型 -------------------\n",
    "from sklearn.metrics import roc_curve\n",
    "from models.mmad_encoder import ImageEncoder, ImageEncoder_CEN  # 根据你的命名调整\n",
    "from models.mmad_encoder import MultiModalClassifier\n",
    "def generate_image_model(cfg):\n",
    "    # 使用 CEN 版本的编码器 + 分类头\n",
    "    model = ImageEncoder_CEN(\n",
    "        in_ch_modality   = 1,\n",
    "        level_channels   = [64, 128, 256],\n",
    "        bottleneck_ch    = 512,\n",
    "        share_layers     = 2,\n",
    "        cen_ratios       = (0.20, 0.10),\n",
    "    ).to(cfg.device)\n",
    "\n",
    "    # 参数统计\n",
    "    total_params     = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    bytes_per_param  = 2 if getattr(cfg, 'fp16', False) else 4\n",
    "\n",
    "    print(\"-------------------- model --------------------\")\n",
    "    print(f\"Total params(M)    : {total_params:,}\")\n",
    "    print(f\"Trainable params(M): {trainable_params:,}\")\n",
    "    print(f\"Approx. size       : {total_params * bytes_per_param / 1024**2:.2f} MB\")\n",
    "    print(\"Model type:\", type(model).__name__)\n",
    "\n",
    "    return model\n",
    "\n",
    "def generate_mm_classifier(cfg):\n",
    "    model = MultiModalClassifier(\n",
    "        img_dim=1024,\n",
    "        tab_dim=192,\n",
    "        num_classes=2\n",
    "    ).to(cfg.device)\n",
    "    # 参数统计\n",
    "    total_params     = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    bytes_per_param  = 2 if getattr(cfg, 'fp16', False) else 4\n",
    "\n",
    "    print(\"-------------------- model --------------------\")\n",
    "    print(f\"Total params(M)    : {total_params:,}\")\n",
    "    print(f\"Trainable params(M): {trainable_params:,}\")\n",
    "    print(f\"Approx. size       : {total_params * bytes_per_param / 1024**2:.2f} MB\")\n",
    "    print(\"Model type:\", type(model).__name__)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = generate_image_model(cfg)\n",
    "model = generate_mm_classifier(cfg)\n",
    "print(model)\n",
    "\n",
    "# -------------------- 测试 -----------------------------\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def load_test_data(cfg, fold):\n",
    "    full_ds = ADNI(\n",
    "        cfg.label_file,\n",
    "        cfg.mri_dir,\n",
    "        cfg.pet_dir,\n",
    "        cfg.task,\n",
    "        cfg.augment\n",
    "    ).data_dict\n",
    "\n",
    "    idx_path = os.path.join(cfg.checkpoint_dir, \"fold_indices.json\")\n",
    "    with open(idx_path, \"r\") as f:\n",
    "        all_indices = json.load(f)\n",
    "\n",
    "    test_idx = all_indices[str(fold)][\"test_idx\"]\n",
    "    test_data = [full_ds[i] for i in test_idx]\n",
    "    return test_data\n",
    "\n",
    "def test_models(checkpoint_dir: str, test_data: list, fold: int):\n",
    "    \"\"\"\n",
    "    对 Dual-Stream UNet3D + 多模态分类器 进行单折测试\n",
    "    返回:\n",
    "        metrics : dict   — calculate_metrics 输出\n",
    "        y_prob  : list   — 正类概率\n",
    "        y_true  : list   — 真实标签\n",
    "        y_pred  : list   — 0/1 预测标签\n",
    "    \"\"\"\n",
    "    device = cfg.device\n",
    "\n",
    "    # ---------- DataLoader ----------\n",
    "    _, test_tf = ADNI_transform(augment=False)\n",
    "    from monai.data import Dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "    ds = Dataset(data=test_data, transform=test_tf)\n",
    "    loader = DataLoader(ds,\n",
    "                        batch_size=cfg.batch_size,\n",
    "                        shuffle=False,\n",
    "                        num_workers=2,\n",
    "                        pin_memory=True)\n",
    "\n",
    "    # ---------- 构造模型结构 ----------\n",
    "    img_encoder = generate_image_model(cfg)      # Dual-Stream UNet3D encoder\n",
    "    clf_model   = generate_mm_classifier(cfg)    # 影像特征 + 表格分类头\n",
    "\n",
    "    # ---------- 加载最佳权重 ----------\n",
    "    ckpt_path = os.path.join(checkpoint_dir, f\"best_model_fold{fold}.pth\")\n",
    "    assert os.path.exists(ckpt_path), f\"❌ 找不到 {ckpt_path}\"\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    img_encoder.load_state_dict(ckpt[\"img_encoder\"])\n",
    "    clf_model.load_state_dict(ckpt[\"clf_model\"])\n",
    "    img_encoder.to(device).eval()\n",
    "    clf_model.to(device).eval()\n",
    "    print(f\"✅ Loaded checkpoint from {ckpt_path}\")\n",
    "\n",
    "    # ---------- 推理 ----------\n",
    "    y_true, y_prob = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            mri  = batch[\"MRI\"].to(device)\n",
    "            pet  = batch[\"PET\"].to(device)\n",
    "\n",
    "            # 若测试样本缺少表格特征，则用 0 向量占位\n",
    "            if \"table\" in batch:\n",
    "                table = batch[\"table\"].to(device).float()\n",
    "            else:\n",
    "                table = torch.zeros(\n",
    "                    (mri.size(0), cfg.table_dim), device=device, dtype=torch.float32\n",
    "                )\n",
    "\n",
    "            label = batch[\"label\"].to(device).long()\n",
    "\n",
    "            with autocast(device_type=\"cuda\", enabled=getattr(cfg, \"fp16\", False)):\n",
    "                img_feat = img_encoder(mri, pet)        # [B, feat_dim]\n",
    "                out      = clf_model(img_feat, table)   # [B, num_cls]\n",
    "\n",
    "            probs = torch.softmax(out, dim=1)[:, 1].cpu().numpy()\n",
    "            y_prob.extend(probs)\n",
    "            y_true.extend(label.cpu().numpy())\n",
    "\n",
    "    # ---------- 评估 ----------\n",
    "    y_pred  = (np.array(y_prob) > 0.5).astype(int)\n",
    "    metrics = calculate_metrics(y_true, y_pred, y_prob)\n",
    "\n",
    "    # ---------- ROC 曲线 ----------\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"AUC={metrics['AUC']:.3f}\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC - Fold {fold}\")\n",
    "    plt.legend()\n",
    "    roc_path = os.path.join(checkpoint_dir, f\"roc_fold{fold}.png\")\n",
    "    plt.savefig(roc_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"✅ ROC curve saved to {roc_path}\")\n",
    "\n",
    "    return metrics, y_prob, y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7c2c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取配置\n",
    "config_path = rf\"config\\config2.json\"\n",
    "cfg = Cfg(load_cfg(config_path))\n",
    "\n",
    "# ----------- 统计模型参数（影像编码器 + 多模态分类器）-----------\n",
    "temp_img_enc = generate_image_model(cfg)     # Dual-Stream UNet3D Encoder\n",
    "temp_clf     = generate_mm_classifier(cfg)   # 表格 + 影像分类头\n",
    "\n",
    "total_params     = sum(p.numel() for p in (*temp_img_enc.parameters(),\n",
    "                                           *temp_clf.parameters()))\n",
    "trainable_params = sum(p.numel() for p in (*temp_img_enc.parameters(),\n",
    "                                           *temp_clf.parameters()) if p.requires_grad)\n",
    "bytes_per_param  = 2 if getattr(cfg, 'fp16', False) else 4\n",
    "approx_size_mb   = total_params * bytes_per_param / 1024 ** 2\n",
    "del temp_img_enc, temp_clf\n",
    "\n",
    "#------------- 文件准备 -------------\n",
    "all_metrics = []\n",
    "all_probs   = []\n",
    "all_labels  = []\n",
    "\n",
    "ckpt_dir = cfg.checkpoint_dir\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "results_txt = os.path.join(ckpt_dir, \"test_results.txt\")\n",
    "result_csv  = os.path.join(ckpt_dir, \"result.csv\")\n",
    "\n",
    "# TXT：模型参数 + 表头\n",
    "with open(results_txt, \"w\") as f:\n",
    "    f.write(\"===== MODEL PARAMETERS =====\\n\")\n",
    "    f.write(f\"Total params       : {total_params}\\n\")\n",
    "    f.write(f\"Trainable params   : {trainable_params}\\n\")\n",
    "    f.write(f\"Approx. size (MB)  : {approx_size_mb:.2f}\\n\\n\")\n",
    "    f.write(\"===== FOLD RESULTS =====\\n\")\n",
    "    f.write(\"Fold\\tACC\\tPRE\\tSEN\\tSPE\\tF1\\tAUC\\tMCC\\n\")\n",
    "\n",
    "# CSV：表头\n",
    "with open(result_csv, \"w\", newline=\"\") as csv_f:\n",
    "    writer = csv.writer(csv_f)\n",
    "    writer.writerow([\n",
    "        \"fold\", \"idx_in_fold\", \"sample_id\",\n",
    "        \"true_label\", \"pred_label\", \"correct\"\n",
    "    ])\n",
    "\n",
    "#------------- 逐折测试 -------------\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, matthews_corrcoef,\n",
    "                             confusion_matrix, roc_curve, auc)   # ← 已经 import confusion_matrix\n",
    "\n",
    "for fold in range(1, cfg.n_splits + 1):\n",
    "    print(f\"\\n=== Testing Fold {fold}/{cfg.n_splits} ===\")\n",
    "\n",
    "    # 直接复用训练阶段构好的 test_loader\n",
    "    te_loader  = fold_loaders[fold - 1][\"test_loader\"]\n",
    "    test_data  = te_loader.dataset.data\n",
    "\n",
    "    metrics, probs, labels, preds = test_models(\n",
    "        ckpt_dir, test_data, fold\n",
    "    )\n",
    "\n",
    "    # ---------- ★ 新增：计算混淆矩阵 ----------\n",
    "    cm = confusion_matrix(labels, preds)                   ### NEW\n",
    "\n",
    "    # Console 输出\n",
    "    print(\n",
    "        f\"Fold {fold} - \"\n",
    "        f\"ACC={metrics['ACC']:.4f}, PRE={metrics['PRE']:.4f}, \"\n",
    "        f\"SEN={metrics['SEN']:.4f}, SPE={metrics['SPE']:.4f}, \"\n",
    "        f\"F1={metrics['F1']:.4f}, AUC={metrics['AUC']:.4f}, \"\n",
    "        f\"MCC={metrics['MCC']:.4f}\"\n",
    "    )\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")                       ### NEW\n",
    "\n",
    "    # ---------- TXT 写入 ----------\n",
    "    with open(results_txt, \"a\") as f:\n",
    "        # ① 先写指标行（保持不变）\n",
    "        f.write(\n",
    "            f\"{fold}\\t\"\n",
    "            f\"{metrics['ACC']:.4f}\\t{metrics['PRE']:.4f}\\t\"\n",
    "            f\"{metrics['SEN']:.4f}\\t{metrics['SPE']:.4f}\\t\"\n",
    "            f\"{metrics['F1']:.4f}\\t{metrics['AUC']:.4f}\\t\"\n",
    "            f\"{metrics['MCC']:.4f}\\n\"\n",
    "        )\n",
    "\n",
    "        # ② 再写混淆矩阵行（NEW）\n",
    "        TN, FP, FN, TP = cm.ravel()               # cm 是 2×2 矩阵\n",
    "        f.write(f\"CM\\t{TN}\\t{FP}\\t{FN}\\t{TP}\\n\")   # 与指标行分开写\n",
    "\n",
    "\n",
    "    # CSV：样本级结果\n",
    "    with open(result_csv, \"a\", newline=\"\") as csv_f:\n",
    "        writer = csv.writer(csv_f)\n",
    "        for idx, (sample_dict, y_t, y_p) in enumerate(\n",
    "                zip(test_data, labels, preds)):\n",
    "            sample_id = (\n",
    "                sample_dict.get(\"subject\")\n",
    "                or os.path.basename(sample_dict.get(\"MRI\", f\"s{idx}\"))\n",
    "            )\n",
    "            writer.writerow([\n",
    "                fold, idx, sample_id,\n",
    "                int(y_t), int(y_p), int(y_t == y_p)\n",
    "            ])\n",
    "\n",
    "    # 汇总\n",
    "    all_metrics.append(metrics)\n",
    "    all_probs.extend(probs)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "\n",
    "#------------- 平均 ROC -------------\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "plt.plot(mean_fpr, interp_tpr, 'b-', lw=2,\n",
    "         label=f'Mean ROC (AUC={roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(os.path.join(ckpt_dir, 'mean_test_roc.png'),\n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "#------------- 汇总指标 -------------\n",
    "print(\"\\n=== Final Test Results (mean ± std) ===\")\n",
    "summary_lines = []\n",
    "for k in ['ACC', 'PRE', 'SEN', 'SPE', 'F1', 'AUC', 'MCC']:\n",
    "    vals = [m[k] for m in all_metrics]\n",
    "    mean_val = np.mean(vals)\n",
    "    std_val  = np.std(vals)\n",
    "    line = f\"{k}: {mean_val:.4f} ± {std_val:.4f}\"\n",
    "    print(line)\n",
    "    summary_lines.append(line)\n",
    "\n",
    "with open(results_txt, \"a\") as f:\n",
    "    f.write(\"\\n===== SUMMARY =====\\n\")\n",
    "    for line in summary_lines:\n",
    "        f.write(line + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
